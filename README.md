# ğŸ¤– Chat Your Docs with AI

---

## ğŸ‡ºğŸ‡¸ English

Chat with your documents! Upload your PDF and Markdown files and have AI-powered Q&A conversations. This project provides an AI assistant that understands your documents and answers your questions using RAG (Retrieval-Augmented Generation) technology.

### ğŸ—ï¸ System Architecture

#### Technology Stack
- **Backend API**: .NET Core 8.0
- **AI/ML Processing**: Python 3.9
- **LLM**: Ollama (llama3.1:8b + nomic-embed-text)
- **Vector Storage**: In-memory (Ollama-based)
- **Message Queue**: Redis
- **Frontend**: HTML/CSS/JavaScript
- **Containerization**: Docker & Docker Compose

#### Service Structure
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend UI   â”‚    â”‚  .NET Core API  â”‚    â”‚  Python Worker  â”‚
â”‚   (Port 80)     â”‚â—„â”€â”€â–ºâ”‚   (Port 8080)   â”‚â—„â”€â”€â–ºâ”‚   (Vectorizer)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                        â”‚
                              â–¼                        â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚     Redis       â”‚    â”‚     Ollama      â”‚
                       â”‚   (Port 6379)   â”‚    â”‚   (Port 11434)  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸš€ Quick Start

#### 1. Requirements
- Docker & Docker Compose
- At least 8GB RAM (for Ollama models)
- At least 10GB disk space

#### 2. Installation
```bash
# Clone the project
git clone <repository-url>
cd chat-your-docs-with-ai

# Set up permissions
chmod +x setup-permissions.sh
./setup-permissions.sh

# Download Ollama models
docker compose up ollama
# In another terminal:
docker exec -it ollama_llm ollama pull llama3.1:8b
docker exec -it ollama_llm ollama pull nomic-embed-text

# Start all services
docker compose up --build
```

#### 3. Access
- **Frontend**: http://localhost:80
- **Backend API**: http://localhost:8080
- **Ollama**: http://localhost:11434
- **Redis**: localhost:6379

### ğŸ“‹ API Endpoints

#### Health Check
```http
GET /
```

**Response:**
```json
{
  "status": "healthy",
  "uptime": "2d 5h 30m 15s",
  "timestamp": "2024-12-20T14:30:22Z",
  "services": {
    "redis": {
      "status": "connected",
      "isConnected": true
    },
    "ollama": {
      "status": "connected",
      "isConnected": true,
      "embeddingModel": "nomic-embed-text",
      "chatModel": "llama3.1:8b",
      "modelsAvailable": true
    },
    "pythonWorker": {
      "status": "active",
      "isActive": true,
      "lastActivity": "2024-12-20T14:25:22Z",
      "timeSinceLastActivity": "5.0 minutes ago",
      "totalLogs": 12
    },
    "frontend": {
      "status": "running",
      "isRunning": true,
      "port": 80,
      "message": "Frontend service is running"
    }
  }
}
```

#### File Upload
```http
POST /api/upload
Content-Type: multipart/form-data

Form Data:
- file: PDF or Markdown file
```

#### Question Answering
```http
POST /api/question/ask
Content-Type: application/json

{
  "question": "What information is available in the documents?",
  "maxResults": 5,
  "minRelevance": 0.5
}
```

#### Document Search
```http
POST /api/question/search
Content-Type: application/json

{
  "query": "search term"
}
```

#### System Status
```http
GET /api/question/status
```

#### Application Logs
```http
GET /api/logs?count=50
```

### ğŸ”„ Workflow (Pipeline)

#### 1. File Upload Process
```
1. User uploads PDF/MD file
   â†“
2. .NET Core validates file type & size
   â†“
3. File saved to /data/uploaded_files/
   â†“
4. File hash calculated (duplicate check)
   â†“
5. Job queued to Redis (if not duplicate)
   â†“
6. Response sent to user
```

#### 2. Vectorization Process (Python Worker)
```
1. Python worker polls Redis queue
   â†“
2. Job picked up from queue
   â†“
3. PDF/MD file parsed (text extraction)
   â†“
4. Text chunked (1000 chars, 200 overlap)
   â†“
5. Each chunk embedded via Ollama
   â†“
6. Vectors stored in memory
   â†“
7. Processing logged to Redis
```

#### 3. Q&A Process (RAG Pipeline)
```
1. User asks question via frontend
   â†“
2. .NET Core receives question
   â†“
3. Question embedded via Ollama
   â†“
4. Similar vectors searched in memory
   â†“
5. Top 5 relevant chunks retrieved
   â†“
6. Context built from chunks
   â†“
7. Context + question sent to Ollama LLM
   â†“
8. AI answer generated
   â†“
9. Response with sources sent to user
```

### ğŸ Python Worker Details

#### Responsibilities
- **File Processing**: Parse PDF and Markdown files
- **Text Chunking**: Split large texts into 1000-character chunks
- **Embedding Generation**: Create vectors via Ollama API
- **Vector Storage**: Manage in-memory vector database
- **Redis Logging**: Log processing statuses

#### Supported File Formats
- **PDF**: PyMuPDF (fitz) + PyPDF2 fallback
- **Markdown**: Markdown parser with HTML to text extraction

### ğŸ—ï¸ .NET Core Backend Details

#### Responsibilities
- **File Upload API**: File upload and validation
- **Question Answering**: RAG pipeline management
- **Vector Search**: Similarity search operations
- **LLM Integration**: Ollama chat completion
- **Redis Management**: Queue and logging
- **Frontend Serving**: Static file serving

### ğŸ¨ Frontend Details

#### Features
- **Modern UI**: Gradient design, responsive layout
- **Real-time Chat**: AJAX API communication
- **Loading States**: Spinner and disabled states
- **Error Handling**: User-friendly error messages
- **Source Attribution**: Source file and chunk information
- **Turkish Language**: Turkish interface

### ğŸ”§ Configuration

#### Environment Variables
```yaml
# .NET Core Backend
STORAGE_TYPE=FileStorage
REDIS_CONNECTION_STRING=redis:6379
OLLAMA_HOST=http://ollama:11434
EMBEDDING_MODEL=nomic-embed-text
CHAT_MODEL=llama3.1:8b

# Python Worker
REDIS_HOST=redis
REDIS_PORT=6379
OLLAMA_HOST=http://ollama:11434
EMBEDDING_MODEL=nomic-embed-text
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
```

### ğŸ“Š Monitoring & Logging

#### Log Events
- **APP_STARTED**: Application started
- **REDIS_CONNECTED**: Redis connection established
- **FILE_UPLOADED**: File uploaded
- **FILE_QUEUED**: File queued
- **FILE_PROCESSING_COMPLETED**: File processed
- **FILE_PROCESSING_ERROR**: Processing error
- **PYTHON_WORKER_STARTED**: Worker started

### ğŸš€ Performance & Scalability

#### Optimizations
- **In-memory storage**: Fast vector access
- **Chunking strategy**: Optimal text processing
- **Async operations**: Non-blocking I/O
- **Connection pooling**: Efficient resource usage
- **Error handling**: Graceful degradation

### ğŸ”’ Security

#### File Security
- **File type validation**: Only PDF/MD
- **File size limits**: Size restrictions
- **Filename sanitization**: Safe file names
- **Duplicate detection**: Hash-based checking

### ğŸ› Troubleshooting

#### Common Issues

##### 1. Ollama Model Not Found
```bash
# Check models
docker exec -it ollama_llm ollama list

# Download model
docker exec -it ollama_llm ollama pull llama3.1:8b
docker exec -it ollama_llm ollama pull snowflake-arctic-embed2
```

##### 2. Redis Connection Error
```bash
# Check Redis status
docker logs redis_cache

# Restart Redis
docker restart redis_cache
```

##### 3. File Permission Error
```bash
# Set permissions
./setup-permissions.sh

# Check Docker volumes
docker volume ls
```

### ğŸ“ˆ Future Improvements

#### Planned Features
- [ ] **Authentication**: User management
- [ ] **Rate Limiting**: API throttling
- [ ] **Persistent Storage**: Vector database
- [ ] **Multi-language**: Multi-language support
- [ ] **File Management**: File deletion/editing
- [ ] **Export Features**: Export results
- [ ] **Analytics**: Usage statistics
- [ ] **WebSocket**: Real-time updates

---

## ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e

DokÃ¼manlarÄ±nÄ±zla konuÅŸun! PDF ve Markdown dosyalarÄ±nÄ±zÄ± yÃ¼kleyin, AI ile soru-cevap yapÄ±n. Bu proje, RAG (Retrieval-Augmented Generation) teknolojisi kullanarak dokÃ¼manlarÄ±nÄ±zÄ± anlayan ve sorularÄ±nÄ±zÄ± yanÄ±tlayan bir AI asistanÄ± sunar.

## ğŸ—ï¸ Sistem Mimarisi

### Teknoloji Stack
- **Backend API**: .NET Core 8.0
- **AI/ML Processing**: Python 3.9
- **LLM**: Ollama (llama3.1:8b + nomic-embed-text)
- **Vector Storage**: In-memory (Ollama-based)
- **Message Queue**: Redis
- **Frontend**: HTML/CSS/JavaScript
- **Containerization**: Docker & Docker Compose

### Servis YapÄ±sÄ±
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend UI   â”‚    â”‚  .NET Core API  â”‚    â”‚  Python Worker  â”‚
â”‚   (Port 80)     â”‚â—„â”€â”€â–ºâ”‚   (Port 8080)   â”‚â—„â”€â”€â–ºâ”‚   (Vectorizer)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                        â”‚
                              â–¼                        â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚     Redis       â”‚    â”‚     Ollama      â”‚
                       â”‚   (Port 6379)   â”‚    â”‚   (Port 11434)  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### 1. Gereksinimler
- Docker & Docker Compose
- En az 8GB RAM (Ollama modelleri iÃ§in)
- En az 10GB disk alanÄ±

### 2. Kurulum
```bash
# Projeyi klonlayÄ±n
git clone <repository-url>
cd chat-your-docs-with-ai

# Ä°zinleri ayarlayÄ±n
chmod +x setup-permissions.sh
./setup-permissions.sh

# Ollama modellerini indirin
docker compose up ollama
# BaÅŸka terminal'de:
docker exec -it ollama_llm ollama pull llama3.1:8b
docker exec -it ollama_llm ollama pull nomic-embed-text

# TÃ¼m servisleri baÅŸlatÄ±n
docker compose up --build
```

### 3. EriÅŸim
- **Frontend**: http://localhost:80
- **Backend API**: http://localhost:8080
- **Ollama**: http://localhost:11434
- **Redis**: localhost:6379

## ğŸ“‹ API Endpoints

### File Upload
```http
POST /api/upload
Content-Type: multipart/form-data

Form Data:
- file: PDF veya Markdown dosyasÄ±
```

**Response:**
```json
{
  "success": true,
  "fileDetails": {
    "fileName": "document.pdf",
    "filePath": "/app/data/uploaded_files/document.pdf",
    "fileSize": 1024000,
    "fileHash": "sha256hash...",
    "isDuplicate": false,
    "duplicateOf": null
  },
  "queueInfo": {
    "queued": true,
    "jobId": "uuid-here"
  }
}
```

### Question Answering
```http
POST /api/question/ask
Content-Type: application/json

{
  "question": "DokÃ¼manlarda ne hakkÄ±nda bilgi var?",
  "maxResults": 5,
  "minRelevance": 0.5
}
```

**Response:**
```json
{
  "success": true,
  "answer": "DokÃ¼manlarÄ±nÄ±zda ÅŸu konular hakkÄ±nda bilgi bulunuyor...",
  "sources": [
    {
      "fileName": "document.pdf",
      "chunkIndex": 2,
      "relevance": 0.95,
      "chunkText": "Ä°lgili metin parÃ§asÄ±..."
    }
  ],
  "confidence": 0.87,
  "processingTime": 1.2,
  "contextLength": 2500,
  "retrievedChunks": 5
}
```

### Document Search
```http
POST /api/question/search
Content-Type: application/json

{
  "query": "arama terimi"
}
```

**Response:**
```json
[
  {
    "fileName": "document.pdf",
    "chunkIndex": 1,
    "relevance": 0.92,
    "chunkText": "Arama sonucu metni...",
    "chunkSize": 856,
    "createdAt": "2024-12-20T14:30:22Z"
  }
]
```

### System Status
```http
GET /api/question/status
```

**Response:**
```json
{
  "isReady": true,
  "timestamp": "2024-12-20T14:30:22Z",
  "services": {
    "RAG": true,
    "Ollama": true
  }
}
```

### Application Logs
```http
GET /api/logs?count=50
```

**Response:**
```json
{
  "success": true,
  "totalCount": 25,
  "logs": [
    {
      "timestamp": "2024-12-20T14:30:22Z",
      "level": "INFO",
      "event": "FILE_PROCESSING_COMPLETED",
      "message": "Dosya baÅŸarÄ±yla iÅŸlendi: document.pdf",
      "details": "Job ID: uuid, Chunks: 8, File Size: 1024000 bytes",
      "fileName": "document.pdf",
      "filePath": "/app/data/uploaded_files/document.pdf",
      "fileSize": 1024000
    }
  ],
  "retrievedAt": "2024-12-20T14:30:22Z"
}
```

### Health Check
```http
GET /
```

**Response:**
```json
{
  "status": "healthy",
  "uptime": "2d 5h 30m 15s",
  "timestamp": "2024-12-20T14:30:22Z",
  "services": {
    "redis": {
      "status": "connected",
      "isConnected": true
    },
    "ollama": {
      "status": "connected",
      "isConnected": true,
      "embeddingModel": "nomic-embed-text",
      "chatModel": "llama3.1:8b",
      "modelsAvailable": true
    },
    "pythonWorker": {
      "status": "active",
      "isActive": true,
      "lastActivity": "2024-12-20T14:25:22Z",
      "timeSinceLastActivity": "5.0 dakika Ã¶nce",
      "totalLogs": 12
    },
    "frontend": {
      "status": "running",
      "isRunning": true,
      "port": 80,
      "message": "Frontend servisi Ã§alÄ±ÅŸÄ±yor"
    }
  }
}
```

## ğŸ”„ Ä°ÅŸ AkÄ±ÅŸÄ± (Pipeline)

### 1. Dosya YÃ¼kleme SÃ¼reci
```
1. User uploads PDF/MD file
   â†“
2. .NET Core validates file type & size
   â†“
3. File saved to /data/uploaded_files/
   â†“
4. File hash calculated (duplicate check)
   â†“
5. Job queued to Redis (if not duplicate)
   â†“
6. Response sent to user
```

### 2. Vectorization SÃ¼reci (Python Worker)
```
1. Python worker polls Redis queue
   â†“
2. Job picked up from queue
   â†“
3. PDF/MD file parsed (text extraction)
   â†“
4. Text chunked (1000 chars, 200 overlap)
   â†“
5. Each chunk embedded via Ollama
   â†“
6. Vectors stored in memory
   â†“
7. Processing logged to Redis
```

### 3. Soru-Cevap SÃ¼reci (RAG Pipeline)
```
1. User asks question via frontend
   â†“
2. .NET Core receives question
   â†“
3. Question embedded via Ollama
   â†“
4. Similar vectors searched in memory
   â†“
5. Top 5 relevant chunks retrieved
   â†“
6. Context built from chunks
   â†“
7. Context + question sent to Ollama LLM
   â†“
8. AI answer generated
   â†“
9. Response with sources sent to user
```

## ğŸ Python Worker DetaylarÄ±

### Sorumluluklar
- **Dosya Ä°ÅŸleme**: PDF ve Markdown dosyalarÄ±nÄ± parse etme
- **Text Chunking**: BÃ¼yÃ¼k metinleri 1000 karakterlik parÃ§alara bÃ¶lme
- **Embedding Generation**: Ollama API ile vector oluÅŸturma
- **Vector Storage**: In-memory vector database yÃ¶netimi
- **Redis Logging**: Ä°ÅŸlem durumlarÄ±nÄ± loglama

### Dosya YapÄ±sÄ±
```
python-worker/
â”œâ”€â”€ worker.py              # Ana worker script
â”œâ”€â”€ document_processor.py  # PDF/MD parsing
â”œâ”€â”€ vector_store.py        # Vector operations
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ Dockerfile            # Container config
```

### Desteklenen Dosya FormatlarÄ±
- **PDF**: PyMuPDF (fitz) + PyPDF2 fallback
- **Markdown**: Markdown parser ile HTML'e Ã§evirip text extraction

### Chunking Stratejisi
- **Chunk Size**: 1000 karakter
- **Overlap**: 200 karakter
- **Strategy**: Kelime sÄ±nÄ±rÄ±nda bÃ¶lme
- **Fallback**: Zorla bÃ¶lme (gerekirse)

## ğŸ—ï¸ .NET Core Backend DetaylarÄ±

### Sorumluluklar
- **File Upload API**: Dosya yÃ¼kleme ve validasyon
- **Question Answering**: RAG pipeline yÃ¶netimi
- **Vector Search**: Similarity search operations
- **LLM Integration**: Ollama chat completion
- **Redis Management**: Queue ve logging
- **Frontend Serving**: Static file serving

### Proje YapÄ±sÄ±
```
backend-api/
â”œâ”€â”€ Controllers/
â”‚   â”œâ”€â”€ UploadController.cs    # File upload
â”‚   â”œâ”€â”€ QuestionController.cs  # RAG operations
â”‚   â”œâ”€â”€ HealthController.cs    # Health check
â”‚   â””â”€â”€ LogsController.cs      # Application logs
â”œâ”€â”€ Services/
â”‚   â”œâ”€â”€ IRedisService.cs       # Redis operations
â”‚   â”œâ”€â”€ RedisService.cs        # Redis implementation
â”‚   â”œâ”€â”€ IFileStorageService.cs # File storage
â”‚   â”œâ”€â”€ FileStorageService.cs  # Local file storage
â”‚   â”œâ”€â”€ IOllamaService.cs      # Ollama integration
â”‚   â”œâ”€â”€ OllamaService.cs       # Ollama implementation
â”‚   â”œâ”€â”€ IRagService.cs         # RAG pipeline
â”‚   â””â”€â”€ RagService.cs          # RAG implementation
â”œâ”€â”€ Models/
â”‚   â”œâ”€â”€ LogEntry.cs            # Log model
â”‚   â”œâ”€â”€ UploadResponse.cs      # Upload response
â”‚   â””â”€â”€ QuestionModels.cs      # RAG models
â””â”€â”€ Program.cs                 # Startup configuration
```

### Dependency Injection
```csharp
// Redis Service
builder.Services.AddSingleton<IRedisService, RedisService>();

// File Storage
builder.Services.AddScoped<IFileStorageService, FileStorageService>();

// HTTP Clients
builder.Services.AddHttpClient<IOllamaService, OllamaService>();

// RAG Services
builder.Services.AddScoped<IOllamaService, OllamaService>();
builder.Services.AddScoped<IRagService, RagService>();
```

## ğŸ¨ Frontend DetaylarÄ±

### Ã–zellikler
- **Modern UI**: Gradient design, responsive layout
- **Real-time Chat**: AJAX ile API communication
- **Loading States**: Spinner ve disabled states
- **Error Handling**: KullanÄ±cÄ± dostu hata mesajlarÄ±
- **Source Attribution**: Kaynak dosya ve chunk bilgileri
- **Turkish Language**: TÃ¼rkÃ§e arayÃ¼z

### Teknoloji
- **HTML5**: Semantic markup
- **CSS3**: Flexbox, gradients, animations
- **Vanilla JavaScript**: No frameworks, lightweight
- **Fetch API**: Modern HTTP requests

## ğŸ”§ KonfigÃ¼rasyon

### Environment Variables
```yaml
# .NET Core Backend
STORAGE_TYPE=FileStorage
REDIS_CONNECTION_STRING=redis:6379
OLLAMA_HOST=http://ollama:11434
EMBEDDING_MODEL=nomic-embed-text
CHAT_MODEL=llama3.1:8b

# Python Worker
REDIS_HOST=redis
REDIS_PORT=6379
OLLAMA_HOST=http://ollama:11434
EMBEDDING_MODEL=nomic-embed-text
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
```

### Docker Compose Services
```yaml
services:
  redis:          # Message queue & logging
  rag-backend-api: # .NET Core API
  worker:         # Python vectorizer
  ollama:         # LLM & embeddings
  frontend:       # Web interface
```

## ğŸ“Š Monitoring & Logging

### Log Events
- **APP_STARTED**: Uygulama baÅŸlatÄ±ldÄ±
- **REDIS_CONNECTED**: Redis baÄŸlantÄ±sÄ± kuruldu
- **FILE_UPLOADED**: Dosya yÃ¼klendi
- **FILE_QUEUED**: Dosya kuyruÄŸa eklendi
- **FILE_PROCESSING_COMPLETED**: Dosya iÅŸlendi
- **FILE_PROCESSING_ERROR**: Ä°ÅŸleme hatasÄ±
- **PYTHON_WORKER_STARTED**: Worker baÅŸlatÄ±ldÄ±

### Health Monitoring
- **Uptime tracking**: Uygulama Ã§alÄ±ÅŸma sÃ¼resi
- **Redis connection**: BaÄŸlantÄ± durumu
- **Service status**: TÃ¼m servislerin durumu
- **Vector stats**: Vector database istatistikleri

## ğŸš€ Performance & Scalability

### Optimizasyonlar
- **In-memory storage**: HÄ±zlÄ± vector access
- **Chunking strategy**: Optimal text processing
- **Async operations**: Non-blocking I/O
- **Connection pooling**: Efficient resource usage
- **Error handling**: Graceful degradation

### Scalability
- **Horizontal scaling**: Multiple Python workers
- **Load balancing**: Multiple .NET instances
- **Redis clustering**: Distributed queue
- **Ollama scaling**: Multiple model instances

## ğŸ”’ GÃ¼venlik

### File Security
- **File type validation**: Sadece PDF/MD
- **File size limits**: Boyut sÄ±nÄ±rlamasÄ±
- **Filename sanitization**: GÃ¼venli dosya adlarÄ±
- **Duplicate detection**: Hash-based checking

### API Security
- **Input validation**: Request validation
- **Error handling**: GÃ¼venli hata mesajlarÄ±
- **Rate limiting**: API rate limiting (gelecek)
- **CORS configuration**: Cross-origin setup

## ğŸ› Troubleshooting

### YaygÄ±n Sorunlar

#### 1. Ollama Model BulunamadÄ±
```bash
# Modelleri kontrol et
docker exec -it ollama_llm ollama list

# Model indir
docker exec -it ollama_llm ollama pull llama3.1:8b
docker exec -it ollama_llm ollama pull nomic-embed-text
```

#### 2. Redis BaÄŸlantÄ± HatasÄ±
```bash
# Redis durumunu kontrol et
docker logs redis_cache

# Redis'i yeniden baÅŸlat
docker restart redis_cache
```

#### 3. Dosya Ä°zin HatasÄ±
```bash
# Ä°zinleri ayarla
./setup-permissions.sh

# Docker volumes'u kontrol et
docker volume ls
```

#### 4. Memory YetersizliÄŸi
```bash
# Memory kullanÄ±mÄ±nÄ± kontrol et
docker stats

# Ollama memory limit ayarla
docker compose down
# docker-compose.yml'de memory limit ekle
docker compose up
```

### Debug KomutlarÄ±
```bash
# TÃ¼m servislerin durumu
docker compose ps

# Log'larÄ± takip et
docker compose logs -f

# Belirli servisin log'u
docker logs rag_worker --tail 50

# Redis iÃ§eriÄŸini kontrol et
docker exec -it redis_cache redis-cli
> KEYS *
> LLEN application_logs
```

## ğŸ“ˆ Gelecek GeliÅŸtirmeler

### Planlanan Ã–zellikler
- [ ] **Authentication**: User management
- [ ] **Rate Limiting**: API throttling
- [ ] **Persistent Storage**: Vector database
- [ ] **Multi-language**: Ã‡oklu dil desteÄŸi
- [ ] **File Management**: Dosya silme/dÃ¼zenleme
- [ ] **Export Features**: SonuÃ§larÄ± export etme
- [ ] **Analytics**: KullanÄ±m istatistikleri
- [ ] **WebSocket**: Real-time updates

### Teknik Ä°yileÅŸtirmeler
- [ ] **Caching**: Response caching
- [ ] **Compression**: Gzip compression
- [ ] **Monitoring**: Prometheus/Grafana
- [ ] **Testing**: Unit & integration tests
- [ ] **CI/CD**: Automated deployment
- [ ] **Documentation**: API documentation

## ğŸ“„ Lisans

Bu proje MIT lisansÄ± altÄ±nda lisanslanmÄ±ÅŸtÄ±r.

## ğŸ¤ KatkÄ±da Bulunma

1. Fork yapÄ±n
2. Feature branch oluÅŸturun (`git checkout -b feature/amazing-feature`)
3. Commit yapÄ±n (`git commit -m 'Add amazing feature'`)
4. Push yapÄ±n (`git push origin feature/amazing-feature`)
5. Pull Request oluÅŸturun

## ğŸ“ Ä°letiÅŸim

- **Proje**: Chat Your Docs with AI
- **Versiyon**: 1.0.0
- **Son GÃ¼ncelleme**: 2024-12-20

---

**Not**: Bu sistem development ortamÄ± iÃ§in tasarlanmÄ±ÅŸtÄ±r. Production kullanÄ±mÄ± iÃ§in ek gÃ¼venlik ve performans optimizasyonlarÄ± gerekebilir.
